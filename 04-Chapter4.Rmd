# Statistical Analysis
##  Regression
### Linear regression on  groups

```{r include=FALSE}
library(Hmisc) #impute values
library(naniar) # deal with NAs
library(geosphere)
library(tidyverse) # data handling and viz
library(janitor) #dataframe import cleaning
library(knitr) #nice html tables
library(kableExtra) # nicer knitr tables
library(broom)
library(readr) # load csv stored data
library(geosphere) # for calc daylength
library(effectsize)
library(factoextra)
library(rstatix)
library(caret)
library(e1071)
```

## Linear Regression
```{r}
kable(mtcars %>%  group_by(as.factor(gear)) %>%
summarise(mean = mean(qsec), sd = sd(qsec))) %>%
  kable_styling(full_width = F) %>%
  kable_minimal()

#Run the same linear regression model by group levels? 
#Instead of running #summary(lm(y~x)) for the number of levels 
#you have, you can use the R package “broom” along with dplyr.

# Run the same regression model for gears ##
kable(mtcars%>% group_by(gear) %>%
  do(fitgear = glance(lm(hp~qsec, data = .))) %>% 
  unnest(fitgear),digits=2) %>%   kable_styling(full_width = F) %>% 
  kable_minimal()
```
[Reference](https://stackoverflow.com/questions/22713325/fitting-several-regression-models-with-dplyr)

```{r}
fit <- lm(qsec ~ wt + hp+disp+factor(cyl), data = mtcars)
summary(fit)
effectsize(fit)
anova_table <- anova(fit)
effectsize(anova_table)
```



## Logistic Regression
### Create the LogR model
```{r}

```

## Principle Component Analysis

*complete dataset needed for following - impute values if required* 
### make the PCA model
```{r}
data("mtcars")
mtcars <- mtcars %>% rownames_to_column("car_name") # if needed for df
mtcars <- rowid_to_column(mtcars, "row_num")
#make the PCA model
pcamodel.pca <- prcomp(mtcars[,c(3:13)], center = TRUE,scale. = TRUE) # I prefer naming columns, but hard with PCA
#summarise PCA
summary(pcamodel.pca)
head(mtcars[,c(3:13)]) #check columns are corrrect
```

### make a scree plot of the PCA model
```{r}
#creates scree plot
fviz_eig(pcamodel.pca)
```
scree plot shows how much  variance can be summarised with one variable (components)

### Graph of variables. Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph.

```{r}
fviz_pca_var(pcamodel.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE   )
```
# Extract the Principal components For each individual

 this is the key stage of getting the first principal component out  per rows score.  essentially doing the principal component creates a method for then taking an rows scores,  and giving them a value for '1 PC'.   then reattach them to the original data frame spreadsheet.
```{r}
extractedpcas <- predict(pcamodel.pca, newdata = mtcars)
head(extractedpcas)
```

*  these are the  seven principal components of the first six people.  we are only interested really in PC1. 
```{r}
# make the Principal component matrix into a data frame 
extractedpcasdf <- as.data.frame(extractedpcas)
extractedpcasdf <- rowid_to_column(extractedpcasdf , "row_num")
#add the principal component values for the individuals to masterPCA file
mtcars <- left_join(mtcars,extractedpcasdf,by="row_num")
```
*this was to add a row number that corresponds to the row number in the cognition data file above.*
* doing this then allows us to add the principal component data to the cognition data and match the stradl IDs.

create a final spreadsheet with cog data and PCA
```{r}
#delete row_num

mtcars<- mtcars%>% select(-c(row_num, PC3:PC11))
mtcars %>% ggplot(aes(x=hp,y=PC1))+geom_point()

```

## Survival Analysis and Visualisation
### To be completed

## Receiver Operated Curves (ROC)
### To be completed

## Missing Values 
```{r}
library(naniar)
library(UpSetR)
#df_cog_w3w4$block3 <- with(df_cog_w3w4, impute(block3, mean))
#df_cog_w3w4$o3tco <- with(df_cog_w3w4, impute(o3tco, mean))
```


### replaces all missing values with mean of column (across all variable columns)
```{r}
# Assuming 'my_data' is your data frame with missing values
# Calculate column means
#means <- colMeans(abc36, na.rm = TRUE)

# Replace missing values in each column with the corresponding column mean
#for (col in names(abc36)) {
#  abc36[is.na(abc36[, col]), col] <- means[col]
#}
```

## Pivot data longer
```{r}
mtcars_long <- mtcars %>% pivot_longer(PC1:PC2, names_to = 'PC', values_to = 'value')
```

## Do analysis on multiple groups (e.g. dependent variable are many shape PCs) rstatix
```{r}
stat.test <- mtcars_long %>% 
  group_by(PC) %>%
  anova_test(value ~ hp+drat) %>%
  adjust_pvalue(method = "BH") %>%
  add_significance()
stat.test
```

## Machine Learning

### Simple example
#### Load example data
```{r}
data("iris")
iris <- iris %>% clean_names()
```

#### Creating random test and train subsets
```{r}
#make this example reproducible
set.seed(1)

#create ID column
iris$id <- 1:nrow(iris)

#use 70% of dataset as training set and 30% as test set 
train_iris <- iris %>% dplyr::sample_frac(0.70)
test_iris  <- dplyr::anti_join(iris, train_iris, by = 'id')


train_iris <- train_iris %>% select(species, sepal_length:petal_width)
test_iris <- test_iris %>% select(species, sepal_length:petal_width)
```

#### create svm iris model
```{r}
iris_svm_model <- svm(species~., data = train_iris, kernel = "linear")
print(iris_svm_model)
```
#### Plot results
```{r}
plot(iris_svm_model,petal_length~petal_width, data = train_iris)
```
#### Test SVM model on test set
```{r}
test_iris <- test_iris %>%
  mutate(pred = predict(iris_svm_model, newdata = ., type = "class"))
```

#### Percent accuracy
```{r}
confusionMatrix(test_iris$pred, test_iris$species)
```

### Clear environment
```{r}
rm(list=setdiff(ls(), "clean environment"))
data("iris")
iris <- as_tibble(iris)
iris <- iris %>% clean_names()
```

### caret to choose machine learning method
```{r}
set.seed(123)  # for reproducibility
train_index_iris <- createDataPartition(iris$species, p = 0.8, list = FALSE)
train_data <- iris[train_index_iris, ]
test_data <- iris[-train_index_iris, ]
```


```{r}
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
```


```{r}
# train the LVQ model
set.seed(7)
modelLvq <- train(species~., data=train_data, method="lvq", trControl=control)
# train the GBM model
set.seed(7)
modelGbm <- train(species~., data=train_data, method="gbm", trControl=control, verbose=FALSE)
# train the SVM model
set.seed(7)
modelSvm <- train(species~., data=train_data, method="svmRadial", trControl=control)
```


```{r}
# collect resamples
results <- resamples(list(LVQ=modelLvq, GBM=modelGbm, SVM=modelSvm))
# summarize the distributions
summary(results)
# boxplots of results
bwplot(results)
# dot plots of results
dotplot(results)
```

### Use caret to tune the parameters of machine learning
#### Clear environment
```{r}
rm(list=setdiff(ls(), "clean environment"))
data("iris")
iris <- as_tibble(iris)
iris <- iris %>% clean_names()
```

### caret to choose machine learning method
```{r}
set.seed(123)  # for reproducibility
train_index_iris <- createDataPartition(iris$species, p = 0.8, list = FALSE)
train_data <- iris[train_index_iris, ]
test_data <- iris[-train_index_iris, ]
```

#### set the training scheme for a ML method (e.g. svm)
```{r}
# ensure results are repeatable
set.seed(7)

# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
```

```{r}
# design the parameter tuning grid
grid <- expand.grid(C=c(0.1,1,5,10,20,30,40,50,100,200), sigma=c(1,5,10))
# train the model
model <- train(species~., data=iris, method="svmRadial", trControl=control, tuneGrid=grid)
# summarize the model
print(model)
```

```{r}
plot(model)
```

#### Use Caret to show and use the best algorithm
```{r}
print(model$finalModel)
```

```{r}
predictions <- predict.train(model, newdata = test_data)
```

```{r}
confusionMatrix(test_data$species, predictions)
```


